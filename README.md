# MLsecurity_fall22

## Before Midproject

### 11-26 - 11-27
1. Starting from the [colab](https://colab.research.google.com/drive/1W0lE-rA8NNJlFUxvRndx6TeXK7CVKkDg)
2. Adopted a model (?) and the dataset (fair(?) and unfair(?)) and run the code for attention visualization
3. Find suitable datasets/pretrained models. 


### 11-28 - 11-30
1. Retrain on the fair dataset and record our preliminary results.
2. Compare our results in mitigating the bias to that of the reference paper.
3. Begin writing our midproject report.

### 12-1 - 12-2
1. submit the mid project

### Visualization Toolkit
[BertViz](https://github.com/jessevig/bertviz)
[VERB](https://tdavislab.github.io/verb/)

### Datasets
[Datasets1](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)
[Datasets2](https://github.com/pliang279/LM_bias)

### Ref papers
[Paper list](https://github.com/uclanlp/awesome-fairness-papers#bias-visualization)

[Detecting Gender Bias in Transformer-based
Models: A Case Study on BERT](https://arxiv.org/pdf/2110.15733.pdf)

[A Multiscale Visualization of Attention in the Transformer Model](https://arxiv.org/pdf/1906.05714.pdf)

[Towards Understanding and Mitigating Social Biases in Language Models](https://arxiv.org/pdf/2106.13219.pdf)


### Midproject Overleaf
[Click here](https://www.overleaf.com/project/63770d89ce2b7968a82877cc)


### Code
https://colab.research.google.com/drive/1sQ-gRenpZt243B-F__0o4Rz1Lf5L4kZj
