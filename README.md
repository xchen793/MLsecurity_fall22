# MLsecurity_fall22

## Before Midproject

### 11-26 - 11-27
1. Starting from the [colab](https://colab.research.google.com/drive/1W0lE-rA8NNJlFUxvRndx6TeXK7CVKkDg)
2. Adopted a model (?) and the dataset (fair(?) and unfair(?)) and run the code for attention visualization
3. Find suitable datasets/pretrained models. 

[Visualization Toolkit Github](https://github.com/jessevig/bertviz)

[Datasets1](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)
[Datasets2](https://github.com/pliang279/LM_bias)


[Detecting Gender Bias in Transformer-based
Models: A Case Study on BERT](https://arxiv.org/pdf/2110.15733.pdf)


### 11-28
1. Retrain on the fair dataset and record our preliminary results.
2. Compare our results in mitigating the bias to that of the reference paper.

### 11-29

### 11-30

### 12-1


### 12-2
submit the mid project

### Visualization Toolkit
[Visualization Toolkit Github](https://github.com/jessevig/bertviz)

### Datasets
[Datasets1](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)
[Datasets2](https://github.com/pliang279/LM_bias)

### Ref papers
[Detecting Gender Bias in Transformer-based
Models: A Case Study on BERT](https://arxiv.org/pdf/2110.15733.pdf)
